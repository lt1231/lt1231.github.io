<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[如何提升hexo博客的逼格]]></title>
    <url>%2F2018%2F07%2F04%2F%E5%85%B3%E4%BA%8Ehexo%E5%8D%9A%E5%AE%A2%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[github博客hexo-next主题配置百度站点收录管理 Github Pages+Hexo+阿里云域名绑定 hexo高阶教程：想让你的博客被更多的人在搜索引擎中搜到吗？ linHexo NexT 博客后台管理指南 Hexo博客同时部署到GitHub和Coding coding 绑定私有域名 Hexo官网 让谷歌收录我们的博客 谷歌站点管理控制台]]></content>
      <categories>
        <category>-hexo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[浅谈javascript闭包机制]]></title>
    <url>%2F2018%2F04%2F12%2F%E6%B5%85%E8%B0%88javascript%E9%97%AD%E5%8C%85%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[闭包（closure）是学习javascript语言的需要攻克的一个难点，也是特色，在初学js的过程中，我也十分纠结于这个点，不理解何为闭包，各种专业文献上的“闭包”定义非常抽象，越看越糊涂，在研读各路大神的博客之后，犹如醍醐灌顶，豁然开朗， 闭包的概念所谓闭包，就是能够读取其他函数内部变量的函数，在js语言中，只有函数内部的子函数才能读取局部变量，因此可以把闭包简单理解成”定义在一个函数内部的函数”。所以，在本质上，闭包就是将函数内部和函数外部连接起来的一座桥梁。 变量作用域要理解闭包，首先必须理解Javascript特殊的变量作用域。变量的作用域无非就是两种：全局变量和局部变量。Javascript语言的特殊之处，就在于函数内部可以直接读取全局变量。 1234567var n=999; function f1()&#123; alert(n); &#125; f1(); // 999 另一方面，在函数外部自然无法读取函数内的局部变量。 12345function f1()&#123; var n=999; &#125; alert(n); // error 这里有一个地方需要注意，函数内部声明变量的时候，一定要使用var命令。如果不用的话，你实际上声明了一个全局变量！ 1234567function f1()&#123; n=999; &#125; f1(); alert(n); // 999 如何从外部读取局部变量？出于种种原因，我们有时候需要得到函数内的局部变量。但是，前面已经说过了，正常情况下，这是办不到的，只有通过变通方法才能实现。 那就是在函数的内部，再定义一个函数 123456789 function f1()&#123; var n=999; function f2()&#123; alert(n); // 999 &#125; &#125; 在上面的代码中，函数f2就被包括在函数f1内部，这时f1内部的所有局部变量，对f2都是可见的。但是反过来就不行，f2内部的局部变量，对f1就是不可见的。这就是Javascript语言特有的”链式作用域”结构（chain scope），子对象会一级一级地向上寻找所有父对象的变量。所以，父对象的所有变量，对子对象都是可见的，反之则不成立。 既然f2可以读取f1中的局部变量，那么只要把f2作为返回值，我们不就可以在f1外部读取它的内部变量了吗！ 123456789101112131415 function f1()&#123; var n=999; function f2()&#123; alert(n); &#125; return f2; &#125; var result=f1(); result(); // 999 闭包的用途及优点闭包可以用在许多地方。它的最大用处有两个，一个是前面提到的可以读取函数内部的变量，另一个就是让这些变量的值始终保持在内存中。 怎么来理解这句话呢？请看下面的代码。 123456789101112131415161718192021function f1()&#123; var n=999; nAdd=function()&#123;n+=1&#125; function f2()&#123; alert(n); &#125; return f2; &#125; var result=f1(); result(); // 999 nAdd(); result(); // 1000 在这段代码中，result实际上就是闭包f2函数。它一共运行了两次，第一次的值是999，第二次的值是1000。这证明了，函数f1中的局部变量n一直保存在内存中，并没有在f1调用后被自动清除。 为什么会这样呢？原因就在于f1是f2的父函数，而f2被赋给了一个全局变量，这导致f2始终在内存中，而f2的存在依赖于f1，因此f1也始终在内存中，不会在调用结束后，被垃圾回收机制（garbage collection）回收。 这段代码中另一个值得注意的地方，就是”nAdd=function(){n+=1}”这一行，首先在nAdd前面没有使用var关键字，因此nAdd是一个全局变量，而不是局部变量。其次，nAdd的值是一个匿名函数（anonymous function），而这个匿名函数本身也是一个闭包，所以nAdd相当于是一个setter，可以在函数外部对函数内部的局部变量进行操作。 使用闭包的注意点1）由于闭包会使得函数中的变量都被保存在内存中，内存消耗很大，所以不能滥用闭包，否则会造成网页的性能问题，在IE中可能导致内存泄露。解决方法是，在退出函数之前，将不使用的局部变量全部删除。 2）闭包会在父函数外部，改变父函数内部变量的值。所以，如果你把父函数当作对象（object）使用，把闭包当作它的公用方法（Public Method），把内部变量当作它的私有属性（private value），这时一定要小心，不要随便改变父函数内部变量的值。思考题 如果你能理解下面两段代码的运行结果，应该就算理解闭包的运行机制了。 代码片段一。 123456789101112131415 var name = &quot;The Window&quot;; var object = &#123; name : &quot;My Object&quot;, getNameFunc : function()&#123; return function()&#123; return this.name; &#125;; &#125; &#125;; alert(object.getNameFunc()()); 代码片段二。 12345678910111213141516 var name = &quot;The Window&quot;; var object = &#123; name : &quot;My Object&quot;, getNameFunc : function()&#123; var that = this; return function()&#123; return that.name; &#125;; &#125; &#125;; alert(object.getNameFunc()()); 本文转自阮一峰的学习Javascript闭包（Closure）]]></content>
      <categories>
        <category>-javascript</category>
      </categories>
      <tags>
        <tag>-前端开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vue环境的搭建]]></title>
    <url>%2F2018%2F04%2F03%2F%E6%89%8B%E6%8A%8A%E6%89%8B%E6%90%AD%E6%9E%B6Vue%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[文章出处https://segmentfault.com/a/1190000008922234]]></content>
      <categories>
        <category>-js脚本框架收录</category>
      </categories>
      <tags>
        <tag>-前端、vue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前端大佬的博客收集]]></title>
    <url>%2F2018%2F04%2F03%2F%E5%89%8D%E7%AB%AF%E5%A4%A7%E4%BD%AC%E7%9A%84%E5%8D%9A%E5%AE%A2%E6%94%B6%E9%9B%86%2F</url>
    <content type="text"><![CDATA[前端大佬的博客收集https://blog.csdn.net/daimomo000/article/details/62887152]]></content>
      <categories>
        <category>-学习指南</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关于SQL查询效率，100w数据，查询只要1秒]]></title>
    <url>%2F2018%2F04%2F03%2F%E5%85%B3%E4%BA%8ESQL%E6%9F%A5%E8%AF%A2%E6%95%88%E7%8E%87%EF%BC%8C100w%E6%95%B0%E6%8D%AE%EF%BC%8C%E6%9F%A5%E8%AF%A2%E5%8F%AA%E8%A6%811%E7%A7%92%EF%BC%8C%E4%B8%8E%E6%82%A8%E5%88%86%E4%BA%AB_%2F</url>
    <content type="text"><![CDATA[机器情况p4: 2.4内存: 1 Gos: windows 2003数据库: ms sql server 2000目的: 查询性能测试,比较两种查询的性能 SQL查询效率 step by step – setp 1.– 建表create table t_userinfo(userid int identity(1,1) primary key nonclustered,nick varchar(50) not null default ‘’,classid int not null default 0,writetime datetime not null default getdate())go – 建索引create clustered index ix_userinfo_classid on t_userinfo(classid)go – step 2. declare @i intdeclare @k intdeclare @nick varchar(10)set @i = 1while @i&lt;1000000beginset @k = @i % 10set @nick = convert(varchar,@i)insert into t_userinfo(nick,classid,writetime) values(@nick,@k,getdate())set @i = @i + 1end– 耗时 08:27 ，需要耐心等待 – step 3.select top 20 userid,nick,classid,writetime from t_userinfowhere userid not in(select top 900000 userid from t_userinfo order by userid asc) – 耗时 8 秒 ,够长的 – step 4.select a.userid,b.nick,b.classid,b.writetime from(select top 20 a.userid from(select top 900020 userid from t_userinfo order by userid asc) a order by a.userid desc) a inner join t_userinfo b on a.userid = b.useridorder by a.userid asc – 耗时 1 秒，太快了吧，不可以思议 – step 5 where 查询select top 20 userid,nick,classid,writetime from t_userinfowhere classid = 1 and userid not in(select top 90000 userid from t_userinfowhere classid = 1order by userid asc)– 耗时 2 秒 – step 6 where 查询select a.userid,b.nick,b.classid,b.writetime from(select top 20 a.userid from(select top 90000 userid from t_userinfowhere classid = 1order by userid asc) a order by a.userid desc) a inner join t_userinfo b on a.userid = b.useridorder by a.userid asc – 查询分析器显示不到 1 秒. 查询效率分析：子查询为确保消除重复值，必须为外部查询的每个结果都处理嵌套查询。在这种情况下可以考虑用联接查询来取代。如果要用子查询，那就用EXISTS替代IN、用NOT EXISTS替代NOT IN。因为EXISTS引入的子查询只是测试是否存在符合子查询中指定条件的行，效率较高。无论在哪种情况下,NOT IN都是最低效的。因为它对子查询中的表执行了一个全表遍历。 建立合理的索引,避免扫描多余数据，避免表扫描！几百万条数据，照样几十毫秒完成查询. SQL提高查询效率2008-05-12 21:201.对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。 2.应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如：select id from t where num is null可以在num上设置默认值0，确保表中num列没有null值，然后这样查询：select id from t where num=0 3.应尽量避免在 where 子句中使用!=或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描。 4.应尽量避免在 where 子句中使用 or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如：select id from t where num=10 or num=20可以这样查询：select id from t where num=10union allselect id from t where num=20 5.in 和 not in 也要慎用，否则会导致全表扫描，如：select id from t where num in(1,2,3)对于连续的数值，能用 between 就不要用 in 了：select id from t where num between 1 and 3 6.下面的查询也将导致全表扫描：select id from t where name like ‘%abc%’若要提高效率，可以考虑全文检索。 7.如果在 where 子句中使用参数，也会导致全表扫描。因为SQL只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推迟到运行时；它必须在编译时进行选择。然而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描：select id from t where num=@num可以改为强制查询使用索引：select id from t with(index(索引名)) where num=@num 8.应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。如：select id from t where num/2=100应改为:select id from t where num=100*2 9.应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如：select id from t where substring(name,1,3)=’abc’–name以abc开头的idselect id from t where datediff(day,createdate,’2005-11-30’)=0–‘2005-11-30’生成的id应改为:select id from t where name like ‘abc%’select id from t where createdate&gt;=’2005-11-30’ and createdate&lt;’2005-12-1’ 10.不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。 11.在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用，并且应尽可能的让字段顺序与索引顺序相一致。 12.不要写一些没有意义的查询，如需要生成一个空表结构：select col1,col2 into #t from t where 1=0这类代码不会返回任何结果集，但是会消耗系统资源的，应改成这样：create table #t(…) 13.很多时候用 exists 代替 in 是一个好的选择：select num from a where num in(select num from b)用下面的语句替换：select num from a where exists(select 1 from b where num=a.num) 14.并不是所有索引对查询都有效，SQL是根据表中数据来进行查询优化的，当索引列有大量数据重复时，SQL查询可能不会去利用索引，如一表中有字段sex，male、female几乎各一半，那么即使在sex上建了索引也对查询效率起不了作用。 15.索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有必要。 16.应尽可能的避免更新 clustered 索引数据列，因为 clustered 索引数据列的顺序就是表记录的物理存储顺序，一旦该列值改变将导致整个表记录的顺序的调整，会耗费相当大的资源。若应用系统需要频繁更新 clustered 索引数据列，那么需要考虑是否应将该索引建为 clustered 索引。 17.尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连接时会逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。 18.尽可能的使用 varchar/nvarchar 代替 char/nchar ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。 19.任何地方都不要使用 select from t ，用具体的字段列表代替“”，不要返回用不到的任何字段。 20.尽量使用表变量来代替临时表。如果表变量包含大量数据，请注意索引非常有限（只有主键索引）。 21.避免频繁创建和删除临时表，以减少系统表资源的消耗。 22.临时表并不是不可使用，适当地使用它们可以使某些例程更有效，例如，当需要重复引用大型表或常用表中的某个数据集时。但是，对于一次性事件，最好使用导出表。 23.在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log ，以提高速度；如果数据量不大，为了缓和系统表的资源，应先create table，然后insert。 24.如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定。 25.尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。 26.使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效。 27.与临时表一样，游标并不是不可使用。对小型数据集使用 FAST_FORWARD 游标通常要优于其他逐行处理方法，尤其是在必须引用几个表才能获得所需的数据时。在结果集中包括“合计”的例程通常要比使用游标执行的速度快。如果开发时间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好。 28.在所有的存储过程和触发器的开始处设置 SET NOCOUNT ON ，在结束时设置 SET NOCOUNT OFF 。无需在执行存储过程和触发器的每个语句后向客户端发送 DONE_IN_PROC 消息。 29.尽量避免大事务操作，提高系统并发能力。 30.尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理1、避免将字段设为“允许为空”2、数据表设计要规范3、深入分析数据操作所要对数据库进行的操作4、尽量不要使用临时表5、多多使用事务6、尽量不要使用游标7、避免死锁8、要注意读写锁的使用9、不要打开大的数据集10、不要使用服务器端游标11、在程序编码时使用大数据量的数据库12、不要给“性别”列创建索引13、注意超时问题14、不要使用Select 15、在细节表中插入纪录时，不要在主表执行Select MAX(ID)16、尽量不要使用TEXT数据类型17、使用参数查询18、不要使用Insert导入大批的数据19、学会分析查询20、使用参照完整性21、用INNER JOIN 和LEFT JOIN代替Where///////////////////////////////////////////////////////////////////////////////////////////http://blog.sina.com.cn/s/blog_4b3d79a9010006gv.html提高SQL查询效率（要点与技巧）：? 技巧一：问题类型：ACCESS数据库字段中含有日文片假名或其它不明字符时查询会提示内存溢出。解决方法：修改查询语句sql=”select from tablename where column like ‘%”&amp;word&amp;”%’”改为sql=”select * from tablename” rs.filter = “ column like ‘%”&amp;word&amp;”%’”技巧二：问题类型：如何用简易的办法实现类似百度的多关键词查询（多关键词用空格或其它符号间隔）。解决方法：‘//用空格分割查询字符串ck=split(word,” “)‘//得到分割后的数量sck=UBound(ck)sql=”select * tablename where”在一个字段中查询For i = 0 To sckSQL = SQL &amp; tempJoinWord &amp; “(“ &amp; “column like ‘“&amp;ck(i)&amp;”%’)”tempJoinWord = “ and “Next在二个字段中同时查询For i = 0 To sckSQL = SQL &amp; tempJoinWord &amp; “(“ &amp; “column like ‘“&amp;ck(i)&amp;”%’ or “ &amp; _“column1 like ‘“&amp;ck(i)&amp;”%’)”tempJoinWord = “ and “ Next技巧三：大大提高查询效率的几种技巧 尽量不要使用 or，使用or会引起全表扫描，将大大降低查询效率。 经过实践验证，charindex()并不比前面加%的like更能提高查询效率，并且charindex()会使索引失去作用（指sqlserver数据库） column like ‘%”&amp;word&amp;”%’ 会使索引不起作用column like ‘“&amp;word&amp;”%’ 会使索引起作用（去掉前面的%符号）（指sqlserver数据库） ‘%”&amp;word&amp;”%’ 与’”&amp;word&amp;”%’ 在查询时的区别：比如你的字段内容为 一个容易受伤的女人‘%”&amp;word&amp;”%’ ：会通配所有字符串，不论查“受伤”还是查“一个”，都会显示结果。‘“&amp;word&amp;”%’ ：只通配前面的字符串，例如查“受伤”是没有结果的，只有查“一个”，才会显示结果。 字段提取要按照“需多少、提多少”的原则，避免“select *”，尽量使用“select 字段1,字段2,字段3……..”。实践证明：每少提取一个字段，数据的提取速度就会有相应的提升。提升的速度还要看您舍弃的字段的大小来判断。 order by按聚集索引列排序效率最高。一个sqlserver数据表只能建立一个聚集索引，一般默认为ID，也可以改为其它的字段。 为你的表建立适当的索引，建立索引可以使你的查询速度提高几十几百倍。（指sqlserver数据库）? 以下是建立索引与不建立索引的一个查询效率分析：Sqlserver索引与查询效率分析。表 News字段Id：自动编号Title：文章标题Author：作者Content：内容Star：优先级Addtime：时间记录：100万条测试机器：P4 2.8/1G内存/IDE硬盘方案1：主键Id，默认为聚集索引，不建立其它非聚集索引select * from News where Title like ‘%”&amp;word&amp;”%’ or Author like ‘%”&amp;word&amp;”%’ order by Id desc从字段Title和Author中模糊检索，按Id排序查询时间：50秒方案2：主键Id，默认为聚集索引在Title、Author、Star上建立非聚集索引select * from News where Title like ‘“&amp;word&amp;”%’ or Author like ‘“&amp;word&amp;”%’ order by Id desc从字段Title和Author中模糊检索，按Id排序查询时间：2 - 2.5秒方案3：主键Id，默认为聚集索引在Title、Author、Star上建立非聚集索引select * from News where Title like ‘“&amp;word&amp;”%’ or Author like ‘“&amp;word&amp;”%’ order by Star desc从字段Title和Author中模糊检索，按Star排序查询时间：2 秒方案4：主键Id，默认为聚集索引在Title、Author、Star上建立非聚集索引select * from News where Title like ‘“&amp;word&amp;”%’ or Author like ‘“&amp;word&amp;”%’从字段Title和Author中模糊检索，不排序查询时间：1.8 - 2 秒方案5：主键Id，默认为聚集索引在Title、Author、Star上建立非聚集索引select from News where Title like ‘“&amp;word&amp;”%’或select from News where Author like ‘“&amp;word&amp;”%’从字段Title 或 Author中检索，不排序查询时间：1秒? 如何提高SQL语言的查询效率?问：请问我如何才能提高SQL语言的查询效率呢？答：这得从头说起：由于SQL是面向结果而不是面向过程的查询语言，所以一般支持SQL语言的大型关系型数据库都使用一个基于查询成本的优化器，为即时查询提供一个最佳的执行策略。对于优化器，输入是一条查询语句，输出是一个执行策略。 一条SQL查询语句可以有多种执行策略，优化器将估计出全部执行方法中所需时间最少的所谓成本最低的那一种方法。所有优化都是基于用记所使用的查询语句中的where子句，优化器对where子句中的优化主要用搜索参数(Serach Argument)。 搜索参数的核心思想就是数据库使用表中字段的索引来查询数据，而不必直接查询记录中的数据。 带有 =、&lt;、&lt;=、&gt;、&gt;= 等操作符的条件语句可以直接使用索引，如下列是搜索参数： emp_id = “10001” 或 salary &gt; 3000 或 a =1 and c = 7 而下列则不是搜索参数： salary = emp_salary 或 dep_id != 10 或 salary * 12 &gt;= 3000 或 a=1 or c=7 应当尽可能提供一些冗余的搜索参数，使优化器有更多的选择余地。请看以下3种方法： 第一种方法： select employee.emp_name,department.dep_name from department,employee where (employee.dep_id = department.dep_id) and (department.dep_code=”01”) and (employee.dep_code=”01”); 它的搜索分析结果如下： Estimate 2 I/O operations Scan department using primary key for rows where dep_code equals “01” Estimate getting here 1 times Scan employee sequentially Estimate getting here 5 times 第二种方法： select employee.emp_name,department.dep_name from department,employee where (employee.dep_id = department.dep_id) and (department.dep_code=”01”); 它的搜索分析结果如下： Estimate 2 I/O operations Scan department using primary key for rows where dep_code equals “01” Estimate getting here 1 times Scan employee sequentially Estimate getting here 5 times 第一种方法与第二种运行效率相同，但第一种方法最好，因为它为优化器提供了更多的选择机会。 第三种方法： select employee.emp_name,department.dep_name from department,employee where (employee.dep_id = department.dep_id) and (employee.dep_code=”01”); 这种方法最不好，因为它无法使用索引，也就是无法优化……使用SQL语句时应注意以下几点： 1、避免使用不兼容的数据类型。例如，Float和Integer，Char和Varchar，Binary和Long Binary不兼容的。数据类型的不兼容可能使优化器无法执行一些本可以进行的优化操作。例如： select emp_name form employee where salary &gt; 3000; 在此语句中若salary是Float类型的，则优化器很难对其进行优化，因为3000是个整数，我们应在编程时使用3000.0而不要等运行时让DBMS进行转化。 2、尽量不要使用表达式，因它在编绎时是无法得到的，所以SQL只能使用其平均密度来估计将要命中的记录数。 3、避免对搜索参数使用其他的数学操作符。如：select emp_name from employee where salary * 12 &gt; 3000; 应改为： select emp_name from employee where salary &gt; 250; 4、避免使用 != 或 &lt;&gt; 等这样的操作符，因为它会使系统无法使用索引，而只能直接搜索表中的数据。? ORACAL中的应用一个1600万数据表－－短信上行表TBL_SMS_MO结构：CREATE TABLE TBL_SMS_MO(SMS_ID NUMBER,MO_ID VARCHAR2(50),MOBILE VARCHAR2(11),SPNUMBER VARCHAR2(20),MESSAGE VARCHAR2(150),TRADE_CODE VARCHAR2(20),LINK_ID VARCHAR2(50),GATEWAY_ID NUMBER,GATEWAY_PORT NUMBER,MO_TIME DATE DEFAULT SYSDATE);CREATE INDEX IDX_MO_DATE ON TBL_SMS_MO (MO_TIME)PCTFREE 10INITRANS 2MAXTRANS 255STORAGE( INITIAL 1M NEXT 1M MINEXTENTS 1 MAXEXTENTS UNLIMITED PCTINCREASE 0);CREATE INDEX IDX_MO_MOBILE ON TBL_SMS_MO (MOBILE)PCTFREE 10INITRANS 2MAXTRANS 255STORAGE( INITIAL 64K NEXT 1M MINEXTENTS 1 MAXEXTENTS UNLIMITED PCTINCREASE 0); 问题：从表中查询某时间段内某手机发送的短消息，如下SQL语句：SELECT MOBILE,MESSAGE,TRADE_CODE,MO_TIMEFROM TBL_SMS_MOWHERE MOBILE=’130XXXXXXXX’AND MO_TIME BETWEEN TO_DATE(‘2006-04-01’,’YYYY-MM-DD HH24:MI:SS’) AND TO_DATE(‘2006-04-07’,’YYYY-MM-DD HH24:MI:SS’)ORDER BY MO_TIME DESC返回结果大约需要10分钟，应用于网页查询，简直难以忍受。分析：在PL/SQL Developer，点击“Explain Plan”按钮（或F5键），对SQL进行分析，发现缺省使用的索引是IDX_MO_DATE。问题可能出在这里，因为相对于总数量1600万数据来说，都mobile的数据是很少的，如果使用IDX_MO_MOBILE比较容易锁定数据。如下优化：SELECT /+ index(TBL_SMS_MO IDX_MO_MOBILE) / MOBILE,MESSAGE,TRADE_CODE,MO_TIMEFROM TBL_SMS_MOWHERE MOBILE=’130XXXXXXXX’AND MO_TIME BETWEEN TO_DATE(‘2006-04-01’,’YYYY-MM-DD HH24:MI:SS’) AND TO_DATE(‘2006-04-07’,’YYYY-MM-DD HH24:MI:SS’)ORDER BY MO_TIME DESC测试：按F8运行这个SQL，哇～… … 2.360s，这就是差别。用索引提高SQL Server性能特别说明 在微软的SQL Server系统中通过有效的使用索引可以提高数据库的查询性能，但是性能的提高取决于数据库的实现。在本文中将会告诉你如何实现索引并有效的提高数据库的性能。 在关系型数据库中使用索引能够提高数据库性能，这一点是非常明显的。用的索引越多，从数据库系统中得到数据的速度就越快。然而，需要注意的是，用的索引越多，向数据库系统中插入新数据所花费的时间就越多。在本文中，你将了解到微软的SQL Server数据库所支持的各种不同类型的索引，在这里你将了解到如何使用不同的方法来实现索引，通过这些不同的实现方法，你在数据库的读性能方面得到的远比在数据库的整体性能方面的损失要多得多。 索引的定义 索引是数据库的工具，通过使用索引，在数据库中获取数据的时候，就可以不用扫描数据库中的所有数据记录，这样能够提高系统获取数据的性能。使用索引可以改变数据的组织方式，使得所有的数据都是按照相似的结构来组织的，这样就可以很容易地实现数据的检索访问。索引是按照列来创建的，这样就可以根据索引列中的值来帮助数据库找到相应的数据。 索引的类型 微软的SQL Server 支持两种类型的索引：clustered 索引和nonclustered索引。Clustered 索引在数据表中按照物理顺序存储数据。因为在表中只有一个物理顺序，所以在每个表中只能有一个clustered索引。在查找某个范围内的数据时，Clustered索引是一种非常有效的索引，因为这些数据在存储的时候已经按照物理顺序排好序了。 Nonclustered索引不会影响到下面的物理存储，但是它是由数据行指针构成的。如果已经存在一个clustered索引，在nonclustered中的索引指针将包含clustered索引的位置参考。这些索引比数据更紧促，而且对这些索引的扫描速度比对实际的数据表扫描要快得多。 如何实现索引 数据库可以自动创建某些索引。例如，微软的SQL Server系统通过自动创建唯一索引来强制实现UNIQUE约束，这样可以确保在数据库中不会插入重复数据。也可以使用CREATE INDEX语句或者通过SQL Server Enterprise Manager来创建其他索引，SQL Server Enterprise Manager还有一个索引创建模板来指导你如何创建索引。 得到更好的性能 虽然索引可以带来性能上的优势，但是同时也将带来一定的代价。虽然SQL Server系统允许你在每个数据表中创建多达256个nonclustered索引，但是建议不要使用这么多的索引。因为索引需要在内存和物理磁盘驱动器上使用更多的存储空间。在执行插入声明的过程中可能会在一定程度上导致系统性能的下降，因为在插入数据的时候是需要根据索引的顺序插入，而不是在第一个可用的位置直接插入数据，这样一来，存在的索引越多将导致插入或者更新声明所需要的时间就越多。 在使用SQL Server系统创建索引的时候，建议参照下面的创建准则来实现： 正确的选择数据类型 在索引中使用某些数据类型可以提高数据库系统的效率，例如，Int，bigint， smallint，和tinyint等这些数据类型都非常适合于用在索引中，因为他们都占用相同大小的空间并且可以很容易地实现比较操作。其他的数据类型如char和varchar的效率都非常低，因为这些数据类型都不适合于执行数学操作，并且执行比较操作的时间都比上面提到数据类型要长。 确保在使用的过程中正确的利用索引值 在执行查询操作时，可能所使用的列只是clustered的一部分，这时尤其要注意的是如何使用这些数据。当用这些数据列作为参数调用函数时，这些函数可能会使现有的排序优势失效。例如，使用日期值作为索引，而为了实现比较操作，可能需要将这个日期值转换为字符串，这样将导致在查询过程中无法用到这个日期索引值。 在创建多列索引时，需要注意列的顺序 数据库将根据第一列索引的值来排列记录，然后进一步根据第二列的值来排序，依次排序直到最后一个索引排序完毕。哪一列唯一数据值较少，哪一列就应该为第一个索引，这样可以确保数据可以通过索引进一步交叉排序。 在clustered索引中限制列的数量 在clustered索引中用到的列越多，在nonclustered索引中包含的clustered索引参考位置就越多，需要存储的数据也就越多。这样将增加包含索引的数据表的大小，并且将增加基于索引的搜索时间。 避免频繁更新clustered索引数据列 由于nonclustered 索引依赖于clustered 索引，所以如果构成clustered 索引的数据列频繁更新，将导致在nonclustered中存储的行定位器也将随之频繁更新。对于所有与这些列相关的查询来说，如果发生记录被锁定的情况时，这将可能导致性能成本的增加。 分开操作（如果可能的话） 对于一个表来说，如果需要进行频繁的执行插入、更新操作，同时还有大量读操作的话，在可能的情况下尝试将这个表分开操作。所有的插入和更新操作可以在一个没有索引的表中操作，然后将其复制到另外一个表中，在这个表里有大量的索引可以优化读数据的能力。 适当的重建索引 Nonclustered索引包含clustered索引的指针，这样一来Nonclustered索引将从属于clustered 索引。当重建clustered索引时，首先是丢弃原来的索引，然后再使用CREATE INDEX 来创建索引，或者在使用CREATE INDEX 声明的同时将DROP_EXISTING 子句作为重建索引的一部分。将丢弃和创建分为几步将会导致多次重建nonclustered 索引，而不象使用DROP_EXISTING 子句那样，只重建一次nonclustered 索引。 明智的使用填充因子 数据存储在那些具有固定大小的连续内存页面内。随着新的记录行的加入，数据内存页将逐渐被填满，系统就必须执行数据页的拆分工作，通过这个拆分工作将部分数据转移到下一个新的页面当中。这样的拆分之后，将加重系统的负担，并且会导致存储的数据支离破碎。填充因子可以维护数据之间的缺口，一般在创建索引的时候，该索引的填充因子就已经被设置好了。这样一来，可以减少插入数据所引起的页面分裂的次数。因为只是在创建索引的时候才维护空间的大小，在增加数据或者更新数据时不会去维护空间的大小。因此，要想能够充分的利用填充因子，就必须周期性的重建索引。由填充因子所造成的缺口将导致读性能的下降，因为随着数据库的扩张，越来越多的磁盘存取工作需要读取数据。所以，在读的次数超过写的次数的时候，很重要的一点是考虑使用填充因子还是使用缺省方式合适。 管理层的决策 通过有效的使用索引，可以在微软的SQL Server系统中实现很好的查询功能，但是使用索引的效率取决于几种不同的实现决策。在索引的性能平衡方面，要做出正确的数据库管理决策意味着需要在良好的性能和困境中抉择。在特定的情况下，本文给出的一些建议将有助于你做出正确的决策]]></content>
      <categories>
        <category>-sqlserver</category>
      </categories>
      <tags>
        <tag>-后台</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git操作命令总结]]></title>
    <url>%2F2018%2F04%2F02%2Fgit%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[HEAD：当前commit引用 $ git version # → git版本 $ git branch # → 查看本地所有的分支 $ git branch # → 查看本地所有的分支$ git branch -r # → 查看所有远程的分支 $ git branch -a # → 查看所有远程分支和本地分支 $ git branch -d # → 删除本地branchname分支 $ git branch -m brancholdname branchnewname # → 重命名分支 $ git branch # → 创建branchname分支 $ git checkout # → 切换分支到branchname $ git checkout -b # → 等同于执行上两步，即创建新的分支并切换到该分支 $ git checkout – xx/xx # → 回滚单个文件 $ git pull origin master:master # → 将远程origin主机的master分支合并到当前master分支,冒号后面的部分表示当前本地所在的分支 $ git push origin -d # → 删除远程branchname分支 $ git fetch –p # → 更新分支 $ git status # → 查看仓库状态 $ git add xx # → 把xx文件添加到暂存区去 $ git commit -m ‘ ‘ # → 提交文件 -m 后面的是注释(不建议使用?) $ git commit -am(-a -m) # → 提交所有的修改，等同于上两步(不建议使用?) $ git commit ./xx # → 等同于git add ./xx + git commit（建议使用?） $ git commit –amend # → 将暂存区和当前commit合并创建一个新commit去替换当前commit $ git stash # → 把当前的工作隐藏起来 等以后恢复现场后继续工作 $ git stash pop # → 恢复工作现场（恢复隐藏的文件，同时删除stash列表中对应的内容） $ git fetch –all # → 将远程主机的更新全部取回本地 $ git merge origin/master # → 在本地（当前）分支上合并远程分支 $ git merge –abort # → 终止本次merge，并回到merge前的状态（?） $ git pull origin master # → 从远程获取最新版本并merge到本地等同于 $ git fetch origin master + $ git merge origin/master（前者更安全一些） $ git push origin master # → 将本地master分支推送到远程origin主机的master分支 $ git log xx # → 查看xx文件的commit记录 $ git log -p xx # → 查看xx文件每次提交的diff $ git log –pretty=oneline xx # → 查看xx文件提交的历史记录（只显示哈希值和提交说明） $ git log –pretty=raw # → 查看commit之间的父子关系（root commit是没有父提交的） $ git log –graph # → 查看当前分支commit生成的树状图 $ git diff HEAD HEAD^1 – xx # → 查看xx文件不同版本之间的差异 $ git diff HEAD~1 # → 显示父节点的提交 git中‘~’和‘^’的区别： (|HEAD)^n，指的是HEAD的第n个父提交，可以通过在“^”后面跟上一个数字，表示第几个父提交，“^”相当“^1”。例如：HEAD^2 表示HEAD的第二次父提交。(|HEAD)~n，指的是HEAD的第n个祖先提交，可以通过在“~”后面跟上一个数字，表示第几个祖父提交，“~”相当“~1”，“~n”相当于连续的个“^”。例如：HEAD~2 表示HEAD的第一个父提交的第一个父提交。 等式1：HEAD~ === HEAD^ === HEAD^1 等式2：HEAD~2 === HEAD^^ === HEAD^1^1 $ git diff –staged/–cached # → 显示暂存区和上一次提交的不同，git add之前忘diff的后悔药 $ git show –stat # → 查看最后一次的修改 $ git show HEAD # → 查看指定版本的修改（可省略HEAD，默认当前版本） 同上 $ git show HEAD xxx # → 查看指定版本xx文件的修改（可省略HEAD，默认当前版本） $ git reset –hard HEAD # → 回滚到指定版本，同时清空工作目录的所有改动 $ git reset –soft HEAD # → 回滚到指定版本，同时保留工作目录和暂存区的内容，并把重置的位置所导致的新的文件差异放进暂存区 $ git reset –mixed HEAD # → （默认）回滚到指定版本，同时保留工作目录的内容，并清空暂存区 $ git reset –hard origin/master # → 将本地master与远程master同步 –hard –soft –mixed的区别可用下图表示： 假设当前commit和工作目录如下所示：如果这时你执行：git reset –hard HEAD^改动全部消失，未跟踪文件除外⬆️ git show –stat查看此时对应的当前commit⬆️2⃣️如果这时你执行：git reset –soft HEAD^reset之前commit的改动被放进暂存区，并保留了工作目录⬆️ git show –stat查看此时对应的当前commit⬆️如果这时你执行：git reset –mixed HEAD^同–soft一样保留了工作目录，但暂存区被全部被清空，之前commit的改动被放到未追踪文件中⬆️ git show –stat查看此时对应的当前commit⬆️$ git reflog show –date=iso # → 查看分支的创建时间 $ git branch -r | awk ‘{print $1}’ | egrep -v -f /dev/fd/0 &lt;(git branch -vv | grep origin) | awk ‘{print $1}’ | xargs git branch -d # → 删除在远程已被删除的本地分支 (慎用) $ git remote show origin # → 查看remote地址，远程分支，还有本地分支与之相对应关系等信息。 $ git remote prune origin # → 删除了那些远程仓库不存在的分支 === git fetch -p $ git config # → 查看和编辑git的配置查看格式：git config [–local|–global|–system] -l $ git config –local -l # → 查看仓库级的config $ git config –global -l # → 查看全局级的config 编辑格式：git config [–local|–global|–system] -e $ git config –local -e # → 编辑仓库级的config $ git config –global -e # → 编辑全局级的config 修改格式：git config [–local|–global|–system] section.key value $ git config –local push.default ‘simple’ # → 修改仓库级的push.default的默认行为 $ git config –global push.default ‘current’ # → 修改全局级的push.default的默认行为 关于git default配置这里 增加格式:git config [–local|–global|–system] –add section.key value(默认是添加在local配置中) $ git config –add cat.name songhw # → local配置写入 cat.name = songhw $ git config –local –add cat.name songhw # → 等同于上一步 $ git config –global –add cat.name lhammer # → global配置写入 cat.name = lhammer 获取格式：git config [–local|–global|–system] –get section.key(默认是获取local配置中内容) $ git config –get cat.name # → 输出songhw $ git config –local –get cat.name # → 输出结果同上一步 $ git config –global –get cat.name # → 输出lhammer 删除格式：git config [–local|–global|–system] –unset section.key $ git config –local –unset cat.name # → 删除local配置中的cat.name = songhw $ git config –global –unset cat.name # → 删除local配置中的cat.name = lhammer $ git rebase master # → 在当前分支对master执行rebase $ git rebase -i 目标commit # → 修改历史某一次提交 把需要修改的commit对应的操作指令从pick改为edit $ gitrebase –continue # → 接上一步修改完之后，继续rebase $ gitrebase –onto HEAD HEAD^1 # → 撤销指定的commit，即消失在历史中 $ git push origin -f # → 忽略冲突，强制提交 $ git revert HEAD # → 撤销指定的commit（?） git revert和git rebase –onto的区别： git revert会增加一条新的commit，它的内容与指定commit的修改是相反的，两次相互抵消从而达到撤销的效果，并且在commit历史中，会存在两条提交，一条原始commit，一条它的反转commit，而git rebase –onto是直接将commit从历史记录中直接删除。 $ git checkout HEAD(c08de9a) # → c08de9a为brance删除之前所在的位置 $ git checkout -b # → 重新创建，找回删除的分支 注：不再被引用直接或间接指向的commit会在一定的时间被git回收，所以通过reflog操作找回删除的分支一定要及时，不然有可能由于commit被回收导致永远也找不回了$ git tag # → 列出所有tag $ git tag -l version1.* # → 只会列出1.几的版本 $ git tag (version 1.0) # → 创建轻量级的tag $ git tag -a (version1.0) -m ‘first version’ # → 创建带有信息的tag $ git tag -d (version 1.0) # → 删除指定tag $ git checkout (version 1.0) # → 检出指定tag 作者：LHammer链接：https://juejin.im/post/5a2cdfe26fb9a0452936b07f来源：掘金著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。]]></content>
      <categories>
        <category>-github</category>
      </categories>
      <tags>
        <tag>-前端开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何处理嵌套的表格边框]]></title>
    <url>%2F2018%2F03%2F09%2F%E8%A1%A8%E6%A0%BC%E5%B5%8C%E5%A5%97%E8%BE%B9%E6%A1%86%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[外层table与内层table嵌套，内外表格都需边框时，设置“border=1”，但边框会重复，造成某些地方边框粗，有些地方边框细的问题。 解决办法：外表格样式：1&lt;table border=&quot;1&quot; cellspacing=&quot;0&quot; cellpadding=&quot;0&quot; style=&quot;border-collapse: collapse;”&gt; 内表格样式：1&lt;table border=&quot;1&quot; cellspacing=&quot;0&quot; cellpadding=&quot;0&quot; style=&quot;border-collapse: collapse;border-width:0px; border-style:hidden;&quot;&gt; 按照如上设置：嵌套表格看起来就像一个表格一样，非常漂亮！原文地址]]></content>
      <categories>
        <category>-工作笔录</category>
      </categories>
      <tags>
        <tag>-css样式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则验证手机号，邮箱]]></title>
    <url>%2F2018%2F03%2F09%2F%E6%AD%A3%E5%88%99%E9%AA%8C%E8%AF%81%E6%89%8B%E6%9C%BA%E5%8F%B7%E5%92%8C%E9%82%AE%E7%AE%B1%2F</url>
    <content type="text"><![CDATA[手机号验证1^1[3|4|5|7|8][0-9]&#123;9&#125;$ 这是精准的手机号验证格式解释 ：开头数字是1，第二位数字，是3到8的任何一个数字，后面是0到9的任意数字，一共9个字符 邮箱验证1/^[a-z0-9!#$%&amp;&apos;*+\/=?^_`&#123;|&#125;~.-]+@[a-z0-9]([a-z0-9-]*[a-z0-9])?(\.[a-z0-9]([a-z0-9-]*[a-z0-9])?)*$/i 验证手机号或者邮箱 1/（^0&#123;0,1&#125;(13[0-9]|15[7-9]|153|156|18[7-9])[0-9]&#123;8&#125;$）|（^[a-z0-9!#$%&amp;&apos;*+\/=?^_`&#123;|&#125;~.-]+@[a-z0-9]([a-z0-9-]*[a-z0-9])?(\.[a-z0-9]([a-z0-9-]*[a-z0-9])?)*$）/i 正则表达式的两种定义方式： 123var reg = /^0&#123;0,1&#125;(13[0-9]|15[7-9]|153|156|18[7-9])[0-9]&#123;8&#125;$/;var reg = new RegExp(&apos;^0&#123;0,1&#125;(13[0-9]|15[7-9]|153|156|18[7-9])[0-9]&#123;8&#125;$&apos;);reg.test(&apos;123@qq.com&apos;)]]></content>
      <categories>
        <category>-工作笔录</category>
      </categories>
      <tags>
        <tag>-前端开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[下载控件（点击链接下载文件）]]></title>
    <url>%2F2018%2F03%2F09%2F%E4%B8%8B%E8%BD%BD%E6%8E%A7%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[html部分1&lt;a href=&quot;download.php?file=这里写你需要下载的文件地址&quot;&gt;UCdownload&lt;/a&gt; php部分12345678&lt;?php $file = $_GET[&apos;file&apos;];header (&quot;Content-type: octet/stream&quot;); //字节流，下载使用header (&quot;Content-disposition: attachment; filename=&quot;.$file.&quot;;&quot;); //下载的形式，这里是作为附件下载header(&quot;Content-Length: &quot;.filesize($file)); readfile($file); exit; ?&gt;]]></content>
      <categories>
        <category>-工作笔录</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[json和js的互转]]></title>
    <url>%2F2018%2F03%2F09%2Fjson%E5%92%8Cjs%E7%9A%84%E4%BA%92%E8%BD%AC%2F</url>
    <content type="text"><![CDATA[JS对象转json 12var data = new Object();var json_data = JSON.stringify(data); （可用来成转换js数组） json转JS 12var json_data = $.getJSON();var data = $.parseJSON(json_data); .将json转换成js对象的方法： 1var json = eval(&apos;(&apos; + result + &apos;)&apos;); 通过上面这个表达式，就完成了将服务器端响应给客户端的Json格式的字符串解析成了一个Json（格式的）对象，名称为“json”，通过“json.”或者“json[]”的方式便可进行数据访问。]]></content>
      <categories>
        <category>-工作笔录</category>
      </categories>
      <tags>
        <tag>-前端开发、javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[http状态码大全]]></title>
    <url>%2F2018%2F03%2F09%2Fhttp%E7%8A%B6%E6%80%81%E7%A0%81%E5%A4%A7%E5%85%A8%2F</url>
    <content type="text"><![CDATA[XMLHttpRequest.status: 1xx-信息提示这些状态代码表示临时的响应。客户端在收到常规响应之前，应准备接收一个或多个1xx响应。100-继续。101-切换协议。2xx-成功这类状态代码表明服务器成功地接受了客户端请求。200-确定。客户端请求已成功。201-已创建。202-已接受。203-非权威性信息。204-无内容。205-重置内容。206-部分内容。 3xx-重定向客户端浏览器必须采取更多操作来实现请求。例如，浏览器可能不得不请求服务器上的不同的页面，或通过代理服务器重复该请求。301-对象已永久移走，即永久重定向。302-对象已临时移动。304-未修改。307-临时重定向。4xx-客户端错误发生错误，客户端似乎有问题。例如，客户端请求不存在的页面，客户端未提供有效的身份验证信息。400-错误的请求。401-访问被拒绝。IIS定义了许多不同的401错误，它们指明更为具体的错误原因。这些具体的错误代码在浏览器中显示，但不在IIS日志中显示：401.1-登录失败。401.2-服务器配置导致登录失败。401.3-由于ACL对资源的限制而未获得授权。401.4-筛选器授权失败。401.5-ISAPI/CGI应用程序授权失败。401.7–访问被Web服务器上的URL授权策略拒绝。这个错误代码为IIS6.0所专用。403-禁止访问：IIS定义了许多不同的403错误，它们指明更为具体的错误原因：403.1-执行访问被禁止。403.2-读访问被禁止。403.3-写访问被禁止。403.4-要求SSL。403.5-要求SSL128。403.6-IP地址被拒绝。403.7-要求客户端证书。403.8-站点访问被拒绝。403.9-用户数过多。403.10-配置无效。403.11-密码更改。403.12-拒绝访问映射表。403.13-客户端证书被吊销。403.14-拒绝目录列表。403.15-超出客户端访问许可。403.16-客户端证书不受信任或无效。403.17-客户端证书已过期或尚未生效。403.18-在当前的应用程序池中不能执行所请求的URL。这个错误代码为IIS6.0所专用。403.19-不能为这个应用程序池中的客户端执行CGI。这个错误代码为IIS6.0所专用。403.20-Passport登录失败。这个错误代码为IIS6.0所专用。404-未找到。404.0-（无）–没有找到文件或目录。404.1-无法在所请求的端口上访问Web站点。404.2-Web服务扩展锁定策略阻止本请求。404.3-MIME映射策略阻止本请求。405-用来访问本页面的HTTP谓词不被允许（方法不被允许）406-客户端浏览器不接受所请求页面的MIME类型。407-要求进行代理身份验证。412-前提条件失败。413–请求实体太大。414-请求URI太长。415–不支持的媒体类型。416–所请求的范围无法满足。417–执行失败。423–锁定的错误。5xx-服务器错误服务器由于遇到错误而不能完成该请求。500-内部服务器错误。500.12-应用程序正忙于在Web服务器上重新启动。500.13-Web服务器太忙。500.15-不允许直接请求Global.asa。500.16–UNC授权凭据不正确。这个错误代码为IIS6.0所专用。500.18–URL授权存储不能打开。这个错误代码为IIS6.0所专用。500.100-内部ASP错误。501-页眉值指定了未实现的配置。502-Web服务器用作网关或代理服务器时收到了无效响应。502.1-CGI应用程序超时。502.2-CGI应用程序出错。application.503-服务不可用。这个错误代码为IIS6.0所专用。504-网关超时。505-HTTP版本不受支持。FTP1xx-肯定的初步答复这些状态代码指示一项操作已经成功开始，但客户端希望在继续操作新命令前得到另一个答复。110重新启动标记答复。120服务已就绪，在nnn分钟后开始。125数据连接已打开，正在开始传输。150文件状态正常，准备打开数据连接。2xx-肯定的完成答复一项操作已经成功完成。客户端可以执行新命令。200命令确定。202未执行命令，站点上的命令过多。211系统状态，或系统帮助答复。212目录状态。213文件状态。214帮助消息。215NAME系统类型，其中，NAME是AssignedNumbers文档中所列的正式系统名称。220服务就绪，可以执行新用户的请求。221服务关闭控制连接。如果适当，请注销。225数据连接打开，没有进行中的传输。226关闭数据连接。请求的文件操作已成功（例如，传输文件或放弃文件）。227进入被动模式(h1,h2,h3,h4,p1,p2)。230用户已登录，继续进行。250请求的文件操作正确，已完成。257已创建“PATHNAME”。3xx-肯定的中间答复该命令已成功，但服务器需要更多来自客户端的信息以完成对请求的处理。331用户名正确，需要密码。332需要登录帐户。350请求的文件操作正在等待进一步的信息。4xx-瞬态否定的完成答复该命令不成功，但错误是暂时的。如果客户端重试命令，可能会执行成功。421服务不可用，正在关闭控制连接。如果服务确定它必须关闭，将向任何命令发送这一应答。425无法打开数据连接。426Connectionclosed;transferaborted.450未执行请求的文件操作。文件不可用（例如，文件繁忙）。451请求的操作异常终止：正在处理本地错误。452未执行请求的操作。系统存储空间不够。5xx-永久性否定的完成答复该命令不成功，错误是永久性的。如果客户端重试命令，将再次出现同样的错误。500语法错误，命令无法识别。这可能包括诸如命令行太长之类的错误。501在参数中有语法错误。502未执行命令。503错误的命令序列。504未执行该参数的命令。530未登录。532存储文件需要帐户。550未执行请求的操作。文件不可用（例如，未找到文件，没有访问权限）。551请求的操作异常终止：未知的页面类型。552请求的文件操作异常终止：超出存储分配（对于当前目录或数据集）。553未执行请求的操作。不允许的文件名。常见的FTP状态代码及其原因150-FTP使用两个端口：21用于发送命令，20用于发送数据。状态代码150表示服务器准备在端口20上打开新连接，发送一些数据。226-命令在端口20上打开数据连接以执行操作，如传输文件。该操作成功完成，数据连接已关闭。230-客户端发送正确的密码后，显示该状态代码。它表示用户已成功登录。331-客户端发送用户名后，显示该状态代码。无论所提供的用户名是否为系统中的有效帐户，都将显示该状态代码。426-命令打开数据连接以执行操作，但该操作已被取消，数据连接已关闭。530-该状态代码表示用户无法登录，因为用户名和密码组合无效。如果使用某个用户帐户登录，可能键入错误的用户名或密码，也可能选择只允许匿名访问。如果使用匿名帐户登录，IIS的配置可能拒绝匿名访问。550-命令未被执行，因为指定的文件不可用。例如，要GET的文件并不存在，或试图将文件PUT到您没有写入权限的目录]]></content>
      <categories>
        <category>-http协议</category>
      </categories>
      <tags>
        <tag>-服务端、前端开发</tag>
      </tags>
  </entry>
</search>
